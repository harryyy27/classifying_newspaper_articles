{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifaction of IMDB Reviews\n",
    "\n",
    "For this particular project I will use multiple densely connected layers to classify IMDB reviews based on the words they use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from parasite import parasite\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data,test_labels) = imdb.load_data(num_words=10000)\n",
    "print(train_data[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the words have been vectorized and are represented by integer numbers corresponding to their place in the index. By setting num_words to 10000, this will only take the 10000 most frequently occurring words in all training_data. Any words not making the top 10000 will not be included. This vastly simplifies the network. You can see by the words above that this article only includes numbers that are less than 10000. So the example above represents a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{34701: 'fawn', 52006: 'tsukino', 52007: 'nunnery', 16816: 'sonja', 63951: 'vani', 1408: 'woods', 16115: 'spiders', 2345: 'hanging', 2289: 'woody', 52008: 'trawling'}\n",
      "[20, 1, 82, 44, 74, 3370, 9758, 1963, 2596, 234, 91, 5192, 30, 118, 9, 1415, 2, 34, 2672, 9, 1, 3, 2283, 268, 1855, 1002, 1, 16, 91, 209, 2165, 31, 1028, 44, 255, 91, 2581, 4561, 1, 2, 243, 5, 9, 44, 457, 4508, 41, 9, 6, 1521, 53, 1028, 22, 261, 1784, 1, 20, 5, 64, 1, 1212, 8, 2728, 77, 27, 3598, 3, 4761, 21, 40, 1, 18, 3, 115, 428, 4369, 59, 1345, 9273, 91, 2665, 14, 1, 1123, 5679, 8, 19, 1, 4, 3, 181, 1736, 15, 3, 19, 8, 1574, 5, 91, 3749, 2, 9002, 6, 1094, 8, 91, 2096, 54, 411, 39, 378, 134, 1, 19, 6, 35, 3980, 8, 91, 2664, 12, 5047, 3, 240, 4, 716, 4178, 8312, 5047, 2462, 14, 3, 1314, 111, 6332, 1370, 980, 3717, 91, 50, 2457, 3, 19, 12, 80, 3, 240, 4, 3977, 426, 9802, 4, 704, 2, 12, 23, 109, 418, 2584, 36, 5, 5, 2801, 1956, 23, 3, 2364, 4, 18, 9, 502, 43, 1, 93, 5, 4523, 3, 4561, 8, 11, 13, 21, 5, 294, 202, 1, 990, 4, 21, 98, 12, 6, 3344, 3, 635, 53, 5, 260, 1, 61, 8836, 4, 5, 166, 12, 22, 67, 137, 341, 1218, 16, 24, 2013, 32, 7009, 2150, 9493, 1409, 87, 142, 5, 341, 118, 26, 90, 2, 5952, 3, 4561, 234, 767, 175, 5573, 117, 5, 1, 5, 1501, 1, 52, 6128, 35, 11, 9585, 44, 24, 1, 83, 252, 5, 1501, 3, 789, 16, 1, 8488, 51, 56, 2851, 8714, 5, 94, 1, 5289, 7418, 56, 2851, 142, 5, 5, 1501, 1, 4941, 60, 13, 1987, 30, 2, 233, 2, 44, 74, 5975, 31, 38, 341, 701, 14, 91, 23, 272, 4, 36, 12, 4, 34, 20, 24, 4308, 1020, 5, 416, 341, 15, 24, 88, 962, 1184, 5, 1301, 5047, 148, 1, 6737, 8244, 2, 2125, 722, 24, 1754, 415, 2932, 90, 9, 3, 608, 49, 37, 1531, 12, 6, 54, 1204, 2704, 1, 474, 7311, 15, 3, 1109, 3664, 515, 40, 1, 5355, 5731, 5, 3, 651, 6, 43, 8, 1, 20, 1499, 2, 8, 1, 20, 690]\n",
      "[[  20    1   82   44   74 3370 9758 1963 2596  234   91 5192   30  118\n",
      "     9 1415    2   34 2672    9    1    3 2283  268 1855 1002    1   16\n",
      "    91  209 2165   31 1028   44  255   91 2581 4561    1    2  243    5\n",
      "     9   44  457 4508   41    9    6 1521   53 1028   22  261 1784    1\n",
      "    20    5   64    1 1212    8 2728   77   27 3598    3 4761   21   40\n",
      "     1   18    3  115  428 4369   59 1345 9273   91 2665   14    1 1123\n",
      "  5679    8   19    1    4    3  181 1736   15    3   19    8 1574    5\n",
      "    91 3749    2 9002    6 1094    8   91 2096   54  411   39  378  134\n",
      "     1   19    6   35 3980    8   91 2664   12 5047    3  240    4  716\n",
      "  4178 8312 5047 2462   14    3 1314  111 6332 1370  980 3717   91   50\n",
      "  2457    3   19   12   80    3  240    4 3977  426 9802    4  704    2\n",
      "    12   23  109  418 2584   36    5    5 2801 1956   23    3 2364    4\n",
      "    18    9  502   43    1   93    5 4523    3 4561    8   11   13   21\n",
      "     5  294  202    1  990    4   21   98   12    6 3344    3  635   53\n",
      "     5  260    1   61 8836    4    5  166   12   22   67  137  341 1218\n",
      "    16   24 2013   32 7009 2150 9493 1409   87  142    5  341  118   26\n",
      "    90    2 5952    3 4561  234  767  175 5573  117    5    1    5 1501\n",
      "     1   52 6128   35   11 9585   44   24    1   83  252    5 1501    3\n",
      "   789   16    1 8488   51   56 2851 8714    5   94    1 5289 7418   56\n",
      "  2851  142    5    5 1501    1 4941   60   13 1987   30    2  233    2\n",
      "    44   74 5975   31   38  341  701   14   91   23  272    4   36   12\n",
      "     4   34   20   24 4308 1020    5  416  341   15   24   88  962 1184\n",
      "     5 1301 5047  148    1 6737 8244    2 2125  722   24 1754  415 2932\n",
      "    90    9    3  608   49   37 1531   12    6   54 1204 2704    1  474\n",
      "  7311   15    3 1109 3664  515   40    1 5355 5731    5    3  651    6\n",
      "    43    8    1   20 1499    2    8    1   20  690]]\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict((key,value) for (value,key) in word_index.items())\n",
    "\n",
    "print({k: reverse_word_index[k] for k in list(reverse_word_index.keys())[:10]})\n",
    "split_review = parasite.split(' ')\n",
    "encoded_review = list(filter(lambda x: x<10000, [list(reverse_word_index.keys())[list(reverse_word_index.values()).index(word)] for word in split_review  if word in list(reverse_word_index.values())]))\n",
    "print(encoded_review)\n",
    "# for i in encoded_review:\n",
    "#     if len(str(i)) > 4 and i>9999:\n",
    "#         encoded_review.remove(i)\n",
    "np_array = np.asarray([encoded_review])\n",
    "print(np_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for this part I decided to test this on a review of my own for the film Parasite. And created a function to encode it so I could pass it into the neural network. It was of course a positive review :P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    #create zeros np array / matrix of shape (sequences length, dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    #mark a column in each row where the word index corresponds with the column index\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i,sequence]=1\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test= vectorize_sequences(test_data)\n",
    "p_site=vectorize_sequences(np_array)\n",
    "\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to accept a sequence of numbers and turn them into a 2D binary matrix. This is because neural networks only accept tensors. The original word mapping to indices is not enough in this particular case as it may allow for the network to assume a natural order or pattern to these numbers which is not a good thing in this case! Therefore one hot encoding is used in order to remove this relationship and turn the original sequence of numbers into a binary matrix with 10000 dimensions and n words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 7s 270us/step - loss: 0.1458 - acc: 0.8167\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 2s 99us/step - loss: 0.0761 - acc: 0.9108\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 2s 98us/step - loss: 0.0575 - acc: 0.9325\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 2s 96us/step - loss: 0.0476 - acc: 0.9439\n",
      "25000/25000 [==============================] - 3s 101us/step\n",
      "[0.08614516176342964, 0.88376]\n",
      "[[0.7827585]]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='mse',metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(results)\n",
    "\n",
    "print(model.predict(p_site))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see the labels were also vectorized in order to turn them into floating point numbers. In this particular case the final function was a sigmoid which is the function typically used in logistic regression. This is a binary classifier hence the final input layer has only 1 hidden layer. This means it can either be switched on or off (1 or 0) indicating positive or negative in this case. As you can see from the final print, the network predicted Parasite's review was positive which is definitely the case (0.78 rounds up to 1). Logistic regression gives the probability of success or failure so in this case the review had a 78% chance of being positive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python37164bitbasecondaf3e9afc952da4cb8ac0d3fa0ef5802f7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
